---
language:
- en
license: mit
library_name: peft
tags:
- generated_from_trainer
datasets:
- glue
metrics:
- accuracy
- f1
base_model: roberta-large
model-index:
- name: output_2024-11-04 11:11:51.510244
  results:
  - task:
      type: text-classification
      name: Text Classification
    dataset:
      name: GLUE MRPC
      type: glue
      args: mrpc
    metrics:
    - type: accuracy
      value: 0.8897058823529411
      name: Accuracy
    - type: f1
      value: 0.9197860962566845
      name: F1
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# output_2024-11-04 11:11:51.510244

This model is a fine-tuned version of [roberta-large](https://huggingface.co/roberta-large) on the GLUE MRPC dataset.
It achieves the following results on the evaluation set:
- Loss: 0.4460
- Accuracy: 0.8897
- F1: 0.9198
- Combined Score: 0.9047

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0006
- train_batch_size: 32
- eval_batch_size: 8
- seed: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 50.0

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     | Combined Score |
|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|:--------------:|
| 0.4327        | 1.0   | 115  | 0.4072          | 0.8064   | 0.8681 | 0.8372         |
| 0.4687        | 2.0   | 230  | 0.3718          | 0.8235   | 0.8737 | 0.8486         |
| 0.3598        | 3.0   | 345  | 0.2953          | 0.8652   | 0.9033 | 0.8843         |
| 0.3264        | 4.0   | 460  | 0.3098          | 0.8725   | 0.9100 | 0.8913         |
| 0.2959        | 5.0   | 575  | 0.3336          | 0.8725   | 0.9116 | 0.8921         |
| 0.2659        | 6.0   | 690  | 0.2894          | 0.8848   | 0.9165 | 0.9007         |
| 0.2627        | 7.0   | 805  | 0.2950          | 0.8775   | 0.9120 | 0.8947         |
| 0.2767        | 8.0   | 920  | 0.2826          | 0.8971   | 0.9261 | 0.9116         |
| 0.2047        | 9.0   | 1035 | 0.2959          | 0.8873   | 0.9190 | 0.9031         |
| 0.2161        | 10.0  | 1150 | 0.3342          | 0.8775   | 0.9138 | 0.8956         |
| 0.1718        | 11.0  | 1265 | 0.3696          | 0.8897   | 0.9217 | 0.9057         |
| 0.1567        | 12.0  | 1380 | 0.3321          | 0.8873   | 0.9190 | 0.9031         |
| 0.1812        | 13.0  | 1495 | 0.2891          | 0.8897   | 0.9206 | 0.9052         |
| 0.1987        | 14.0  | 1610 | 0.3023          | 0.8897   | 0.9195 | 0.9046         |
| 0.1488        | 15.0  | 1725 | 0.3335          | 0.8873   | 0.9184 | 0.9028         |
| 0.1646        | 16.0  | 1840 | 0.3214          | 0.8775   | 0.9110 | 0.8942         |
| 0.1416        | 17.0  | 1955 | 0.3568          | 0.8799   | 0.9148 | 0.8973         |
| 0.2255        | 18.0  | 2070 | 0.2977          | 0.8995   | 0.9261 | 0.9128         |
| 0.1889        | 19.0  | 2185 | 0.3133          | 0.8824   | 0.9152 | 0.8988         |
| 0.1686        | 20.0  | 2300 | 0.2995          | 0.8995   | 0.9269 | 0.9132         |
| 0.1539        | 21.0  | 2415 | 0.3470          | 0.8799   | 0.9139 | 0.8969         |
| 0.1995        | 22.0  | 2530 | 0.3539          | 0.8971   | 0.9253 | 0.9112         |
| 0.119         | 23.0  | 2645 | 0.3631          | 0.8775   | 0.9129 | 0.8952         |
| 0.1691        | 24.0  | 2760 | 0.3534          | 0.8799   | 0.9145 | 0.8972         |
| 0.115         | 25.0  | 2875 | 0.3941          | 0.8873   | 0.9190 | 0.9031         |
| 0.1367        | 26.0  | 2990 | 0.3516          | 0.8946   | 0.9231 | 0.9088         |
| 0.1314        | 27.0  | 3105 | 0.3977          | 0.8799   | 0.9139 | 0.8969         |
| 0.166         | 28.0  | 3220 | 0.3748          | 0.8775   | 0.9126 | 0.8950         |
| 0.1212        | 29.0  | 3335 | 0.3928          | 0.8897   | 0.9192 | 0.9045         |
| 0.1196        | 30.0  | 3450 | 0.3945          | 0.8971   | 0.9250 | 0.9110         |
| 0.1321        | 31.0  | 3565 | 0.3623          | 0.8971   | 0.9247 | 0.9109         |
| 0.0819        | 32.0  | 3680 | 0.4389          | 0.8873   | 0.9179 | 0.9026         |
| 0.094         | 33.0  | 3795 | 0.3762          | 0.8897   | 0.9195 | 0.9046         |
| 0.1235        | 34.0  | 3910 | 0.3939          | 0.8971   | 0.9236 | 0.9103         |
| 0.0894        | 35.0  | 4025 | 0.4367          | 0.8873   | 0.9170 | 0.9021         |
| 0.0864        | 36.0  | 4140 | 0.4460          | 0.8848   | 0.9168 | 0.9008         |
| 0.0761        | 37.0  | 4255 | 0.4612          | 0.8897   | 0.9204 | 0.9050         |
| 0.1272        | 38.0  | 4370 | 0.4193          | 0.8922   | 0.9211 | 0.9067         |
| 0.076         | 39.0  | 4485 | 0.4142          | 0.8873   | 0.9187 | 0.9030         |
| 0.1139        | 40.0  | 4600 | 0.4220          | 0.8873   | 0.9184 | 0.9028         |
| 0.0986        | 41.0  | 4715 | 0.4070          | 0.8873   | 0.9181 | 0.9027         |
| 0.0801        | 42.0  | 4830 | 0.4345          | 0.8848   | 0.9159 | 0.9004         |
| 0.0764        | 43.0  | 4945 | 0.4371          | 0.8873   | 0.9184 | 0.9028         |
| 0.0655        | 44.0  | 5060 | 0.4554          | 0.8873   | 0.9181 | 0.9027         |
| 0.0646        | 45.0  | 5175 | 0.4631          | 0.8897   | 0.9206 | 0.9052         |
| 0.0801        | 46.0  | 5290 | 0.4508          | 0.8848   | 0.9171 | 0.9010         |
| 0.0717        | 47.0  | 5405 | 0.4423          | 0.8848   | 0.9165 | 0.9007         |
| 0.0867        | 48.0  | 5520 | 0.4402          | 0.8897   | 0.9198 | 0.9047         |
| 0.0817        | 49.0  | 5635 | 0.4455          | 0.8873   | 0.9179 | 0.9026         |
| 0.0485        | 50.0  | 5750 | 0.4460          | 0.8897   | 0.9198 | 0.9047         |


### Framework versions

- PEFT 0.10.0
- Transformers 4.40.1
- Pytorch 2.2.1+cu121
- Datasets 2.16.1
- Tokenizers 0.19.1